export const projectDetailsData = {
	es: {
		"web-server": {
			overview: {
				title: "Descripci√≥n General",
				content: "Este proyecto surgi√≥ de la curiosidad de entender c√≥mo funciona un servidor web a nivel fundamental. En lugar de usar frameworks populares como Express o FastAPI, decid√≠ construir un servidor HTTP desde cero utilizando sockets TCP/IP nativos. El objetivo era comprender cada aspecto del protocolo HTTP, desde el handshake TCP hasta el parseo de requests y la generaci√≥n de responses."
			},
			challenge: {
				title: "El Desaf√≠o",
				content: "El mayor reto fue implementar el manejo concurrente de m√∫ltiples conexiones sin usar librer√≠as de terceros. Tuve que lidiar con sockets bloqueantes vs no-bloqueantes, gesti√≥n de buffers, y el ciclo de vida completo de una conexi√≥n HTTP. Tambi√©n aprend√≠ sobre los l√≠mites del sistema operativo en cuanto a file descriptors y c√≥mo afectan la escalabilidad."
			},
			solution: {
				title: "La Soluci√≥n",
				content: "Implement√© un servidor multi-threaded que acepta conexiones en el thread principal y las delega a workers para su procesamiento. Cada worker parsea el request HTTP manualmente, identifica el m√©todo y path, y genera una response apropiada. El servidor soporta GET y POST requests, servir archivos est√°ticos, y enviar diferentes status codes seg√∫n sea necesario."
			},
			technical: {
				title: "Detalles T√©cnicos",
				items: [
					"Implementaci√≥n de sockets TCP/IP usando la librer√≠a est√°ndar",
					"Parseo manual del protocolo HTTP/1.1 (request line, headers, body)",
					"Pool de threads para manejo concurrente de conexiones",
					"Gesti√≥n de buffers para lectura y escritura de datos",
					"Implementaci√≥n de Content-Type detection para archivos est√°ticos",
					"Manejo de keep-alive y connection close",
					"Testing con herramientas como curl y Apache Bench"
				]
			},
			learnings: {
				title: "Aprendizajes Clave",
				items: [
					"Comprend√≠ a profundidad el modelo cliente-servidor y el protocolo HTTP",
					"Aprend√≠ sobre programaci√≥n de sistemas y operaciones de I/O a bajo nivel",
					"Entend√≠ las ventajas y limitaciones de diferentes modelos de concurrencia",
					"Gan√© apreciaci√≥n por la complejidad que abstraen los frameworks modernos",
					"Desarroll√© habilidades de debugging a nivel de sistema operativo",
					"Aprend√≠ sobre performance tuning y profiling de aplicaciones de red"
				]
			},
			images: {
				title: "Capturas del Proyecto",
				items: [
					{
						placeholder: "üñ•Ô∏è",
						caption: "Terminal mostrando el servidor aceptando conexiones"
					},
					{
						placeholder: "üìä",
						caption: "Benchmarks de performance con Apache Bench"
					},
					{
						placeholder: "üîç",
						caption: "Request/Response HTTP capturados con Wireshark"
					}
				]
			}
		},
		"authentication": {
			overview: {
				title: "Descripci√≥n General",
				content: "Un sistema de autenticaci√≥n completo que implementa las mejores pr√°cticas de seguridad modernas. El proyecto incluye registro de usuarios, login, gesti√≥n de tokens JWT, refresh tokens, y todas las caracter√≠sticas necesarias para un sistema de autenticaci√≥n robusto en producci√≥n."
			},
			challenge: {
				title: "El Desaf√≠o",
				content: "La autenticaci√≥n es un √°rea cr√≠tica donde los errores tienen consecuencias graves. El desaf√≠o fue implementar un sistema seguro que proteja contra vulnerabilidades comunes como: almacenamiento inseguro de passwords, ataques de timing, session hijacking, CSRF, y XSS. Tambi√©n fue necesario balancear seguridad con experiencia de usuario."
			},
			solution: {
				title: "La Soluci√≥n",
				content: "Implement√© un sistema dual-token: access tokens de corta duraci√≥n para autenticaci√≥n y refresh tokens de larga duraci√≥n almacenados de forma segura. Los passwords se hashean con bcrypt usando salt aleatorio. Los tokens JWT incluyen claims personalizados y est√°n firmados con algoritmos seguros. El sistema tambi√©n implementa rate limiting y protecci√≥n contra ataques de fuerza bruta."
			},
			technical: {
				title: "Detalles T√©cnicos",
				items: [
					"Hashing de passwords con bcrypt (cost factor: 12)",
					"JWT con algoritmo RS256 para firmado asim√©trico",
					"Access tokens (15 min) y Refresh tokens (7 d√≠as)",
					"Refresh token rotation para mayor seguridad",
					"Whitelist de tokens en Redis para revocaci√≥n instant√°nea",
					"Rate limiting por IP y por usuario",
					"Protecci√≥n contra timing attacks en validaci√≥n de passwords",
					"HTTP-only cookies para almacenamiento seguro de tokens",
					"CORS configurado correctamente para prevenir CSRF"
				]
			},
			learnings: {
				title: "Aprendizajes Clave",
				items: [
					"Comprend√≠ las diferencias entre authentication y authorization",
					"Aprend√≠ sobre criptograf√≠a aplicada: hashing, salting, y algoritmos de firmado",
					"Entend√≠ el flujo completo de tokens JWT y sus trade-offs",
					"Aprend√≠ sobre vectores de ataque comunes y c√≥mo mitigarlos",
					"Desarroll√© criterio para evaluar security trade-offs",
					"Gan√© experiencia con Redis para gesti√≥n de sesiones distribuidas"
				]
			},
			images: {
				title: "Capturas del Proyecto",
				items: [
					{
						placeholder: "üîê",
						caption: "Diagrama de flujo del sistema de autenticaci√≥n"
					},
					{
						placeholder: "üé´",
						caption: "Estructura del payload JWT con claims personalizados"
					},
					{
						placeholder: "‚ö†Ô∏è",
						caption: "Rate limiting en acci√≥n bloqueando ataques de fuerza bruta"
					}
				]
			}
		},
		"database": {
			overview: {
				title: "Descripci√≥n General",
				content: "Este proyecto explora el dise√±o y optimizaci√≥n de bases de datos relacionales. Incluye desde el modelado de esquemas normalizados hasta t√©cnicas avanzadas de optimizaci√≥n como indexing, query optimization, y estrategias de caching. El foco est√° en entender c√≥mo las decisiones de dise√±o afectan el performance en aplicaciones reales."
			},
			challenge: {
				title: "El Desaf√≠o",
				content: "Dise√±ar un esquema que sea f√°cil de mantener pero que tambi√©n escale con millones de registros. El reto fue identificar bottlenecks de performance, entender query plans, y aplicar optimizaciones sin sobre-optimizar prematuramente. Tambi√©n aprend√≠ que el caching puede ser una espada de doble filo si no se implementa correctamente."
			},
			solution: {
				title: "La Soluci√≥n",
				content: "Implement√© un esquema normalizado en 3NF con √≠ndices estrat√©gicamente ubicados en columnas que se usan frecuentemente en WHEREs y JOINs. Para queries complejas, uso EXPLAIN ANALYZE para entender el query plan y optimizar en consecuencia. Implement√© caching en m√∫ltiples niveles: query results en Redis y materialized views para agregaciones pesadas."
			},
			technical: {
				title: "Detalles T√©cnicos",
				items: [
					"Dise√±o de esquema normalizado (1NF, 2NF, 3NF) con desnormalizaci√≥n selectiva",
					"√çndices B-tree en columnas de b√∫squeda frecuente",
					"√çndices compuestos para queries multi-columna",
					"Partial indexes para subsets espec√≠ficos de datos",
					"Materialized views para agregaciones complejas",
					"Query optimization usando EXPLAIN ANALYZE",
					"Redis como cache layer con estrategia de invalidaci√≥n",
					"Connection pooling con configuraci√≥n optimizada",
					"Partitioning de tablas grandes por fecha"
				]
			},
			learnings: {
				title: "Aprendizajes Clave",
				items: [
					"Comprend√≠ la teor√≠a de normalizaci√≥n y cu√°ndo romper las reglas",
					"Aprend√≠ a leer query plans y identificar operaciones costosas",
					"Entend√≠ diferentes tipos de √≠ndices y cu√°ndo usar cada uno",
					"Desarroll√© intuici√≥n para el trade-off entre lectura y escritura",
					"Aprend√≠ sobre cache invalidation strategies y sus desaf√≠os",
					"Gan√© experiencia con monitoring y profiling de queries en producci√≥n"
				]
			},
			images: {
				title: "Capturas del Proyecto",
				items: [
					{
						placeholder: "üìê",
						caption: "Diagrama ER del esquema de base de datos"
					},
					{
						placeholder: "üìà",
						caption: "Query performance antes y despu√©s de indexing"
					},
					{
						placeholder: "üîÑ",
						caption: "Arquitectura de caching multi-nivel"
					}
				]
			}
		},
		"message-queue": {
			overview: {
				title: "Descripci√≥n General",
				content: "Implementaci√≥n de un sistema de colas de mensajes para desacoplar componentes y procesar tareas de forma as√≠ncrona. El proyecto demuestra c√≥mo usar message queues para mejorar la escalabilidad y resiliencia de aplicaciones, manejando tareas pesadas como procesamiento de im√°genes, env√≠o de emails, y generaci√≥n de reportes en background."
			},
			challenge: {
				title: "El Desaf√≠o",
				content: "El mayor desaf√≠o fue garantizar la entrega de mensajes sin p√©rdidas ni duplicados. Tuve que lidiar con workers que fallan a mitad de procesamiento, mensajes que causan errores repetidamente (poison messages), y mantener el orden de procesamiento cuando es importante. Tambi√©n fue necesario implementar monitoring para visibilidad del estado de las colas."
			},
			solution: {
				title: "La Soluci√≥n",
				content: "Implement√© el sistema usando RabbitMQ con m√∫ltiples workers que consumen mensajes de diferentes colas. Cada worker procesa mensajes de forma idempotente para manejar redeliveries. Para mensajes que fallan repetidamente, implement√© una estrategia de retry con exponential backoff y una dead letter queue para an√°lisis posterior."
			},
			technical: {
				title: "Detalles T√©cnicos",
				items: [
					"RabbitMQ como message broker con durabilidad habilitada",
					"M√∫ltiples workers con procesamiento concurrente",
					"Acknowledgment manual para garantizar procesamiento",
					"Retry logic con exponential backoff (1s, 2s, 4s, 8s...)",
					"Dead Letter Queue para mensajes que fallan persistentemente",
					"Priority queues para mensajes urgentes",
					"Message persistence en disco para durabilidad",
					"Monitoring con Prometheus y Grafana",
					"Procesamiento idempotente para manejar duplicados"
				]
			},
			learnings: {
				title: "Aprendizajes Clave",
				items: [
					"Comprend√≠ los patrones de messaging: pub/sub, work queues, routing",
					"Aprend√≠ sobre garant√≠as de entrega: at-most-once, at-least-once, exactly-once",
					"Entend√≠ la importancia de la idempotencia en sistemas distribuidos",
					"Desarroll√© estrategias para manejar failures y poison messages",
					"Aprend√≠ a balancear throughput vs latencia en workers",
					"Gan√© experiencia con observability en sistemas as√≠ncronos"
				]
			},
			images: {
				title: "Capturas del Proyecto",
				items: [
					{
						placeholder: "üìÆ",
						caption: "Arquitectura del sistema de message queues"
					},
					{
						placeholder: "üìä",
						caption: "Dashboard de monitoring con m√©tricas de las colas"
					},
					{
						placeholder: "üîÅ",
						caption: "Flujo de retry con exponential backoff"
					}
				]
			}
		},
		"rest-api": {
			overview: {
				title: "Descripci√≥n General",
				content: "Una API RESTful completa siguiendo las mejores pr√°cticas de dise√±o de APIs. El proyecto incluye versionado, rate limiting, autenticaci√≥n, documentaci√≥n interactiva con OpenAPI/Swagger, y un sistema robusto de manejo de errores. El objetivo es crear una API que sea f√°cil de usar, mantener y escalar."
			},
			challenge: {
				title: "El Desaf√≠o",
				content: "Dise√±ar una API que sea intuitiva pero tambi√©n extensible. El reto fue tomar decisiones sobre la estructura de endpoints, versionado, y contratos de datos que no rompan la compatibilidad con clientes existentes. Tambi√©n fue necesario implementar rate limiting efectivo sin afectar usuarios leg√≠timos y crear documentaci√≥n que se mantenga sincronizada con el c√≥digo."
			},
			solution: {
				title: "La Soluci√≥n",
				content: "Implement√© una API siguiendo los principios REST con recursos claramente definidos y m√©todos HTTP sem√°nticamente correctos. El versionado se hace via URL (/v1/, /v2/). Rate limiting est√° implementado con Redis usando sliding window algorithm. La documentaci√≥n se genera autom√°ticamente del c√≥digo usando decorators/annotations, garantizando que siempre est√© actualizada."
			},
			technical: {
				title: "Detalles T√©cnicos",
				items: [
					"Endpoints RESTful con recursos y sub-recursos anidados",
					"Versionado de API via URL path (/api/v1/, /api/v2/)",
					"Rate limiting con sliding window algorithm en Redis",
					"Autenticaci√≥n con Bearer tokens (JWT)",
					"Documentaci√≥n OpenAPI 3.0 generada autom√°ticamente",
					"Swagger UI para testing interactivo",
					"Manejo de errores consistente con Problem Details (RFC 7807)",
					"Paginaci√≥n con cursor-based pagination",
					"HATEOAS para discoverability",
					"Content negotiation (JSON, XML)"
				]
			},
			learnings: {
				title: "Aprendizajes Clave",
				items: [
					"Comprend√≠ los principios de dise√±o REST y sus ventajas",
					"Aprend√≠ sobre API versioning strategies y sus trade-offs",
					"Entend√≠ diferentes algoritmos de rate limiting y sus caracter√≠sticas",
					"Desarroll√© criterio para dise√±ar contratos de API estables",
					"Aprend√≠ la importancia de buena documentaci√≥n y developer experience",
					"Gan√© experiencia con OpenAPI/Swagger para especificaciones de API"
				]
			},
			images: {
				title: "Capturas del Proyecto",
				items: [
					{
						placeholder: "üìù",
						caption: "Swagger UI con documentaci√≥n interactiva"
					},
					{
						placeholder: "üö¶",
						caption: "Rate limiting en acci√≥n protegiendo la API"
					},
					{
						placeholder: "üìã",
						caption: "Ejemplo de respuesta con formato Problem Details"
					}
				]
			}
		},
		"devops": {
			overview: {
				title: "Descripci√≥n General",
				content: "Este proyecto cubre el ciclo completo de deployment y DevOps para aplicaciones modernas. Incluye containerizaci√≥n con Docker, orquestaci√≥n con Kubernetes, CI/CD pipelines automatizados, y monitoring en producci√≥n. El objetivo es entender c√≥mo llevar c√≥digo desde desarrollo hasta producci√≥n de forma automatizada, segura y confiable."
			},
			challenge: {
				title: "El Desaf√≠o",
				content: "El mayor desaf√≠o fue configurar un pipeline completo que sea robusto pero no demasiado complejo. Tuve que aprender sobre m√∫ltiples herramientas (Docker, Kubernetes, CI/CD, monitoring) y c√≥mo integrarlas coherentemente. Tambi√©n fue necesario implementar zero-downtime deployments y rollback autom√°tico cuando algo falla."
			},
			solution: {
				title: "La Soluci√≥n",
				content: "Containeriz√© la aplicaci√≥n con Docker usando multi-stage builds para optimizar tama√±o. Implement√© CI/CD con GitHub Actions que corre tests, build, y deployment autom√°tico. El deployment a Kubernetes usa rolling updates para zero downtime. Para monitoring, configur√© Prometheus para m√©tricas y Grafana para visualizaci√≥n, con alerts autom√°ticos cuando algo sale mal."
			},
			technical: {
				title: "Detalles T√©cnicos",
				items: [
					"Dockerfiles con multi-stage builds para optimizaci√≥n",
					"Docker Compose para desarrollo local con m√∫ltiples servicios",
					"Kubernetes deployment con rolling updates y rollback autom√°tico",
					"Health checks (liveness, readiness probes) para K8s",
					"CI/CD pipeline con GitHub Actions (test ‚Üí build ‚Üí deploy)",
					"GitOps workflow con automated deployments",
					"Prometheus para recolecci√≥n de m√©tricas de aplicaci√≥n",
					"Grafana dashboards para visualizaci√≥n de m√©tricas",
					"Alertmanager para notifications de incidentes",
					"Horizontal Pod Autoscaling basado en m√©tricas"
				]
			},
			learnings: {
				title: "Aprendizajes Clave",
				items: [
					"Comprend√≠ el ciclo completo de desarrollo a producci√≥n",
					"Aprend√≠ sobre containerizaci√≥n y sus ventajas para deployment",
					"Entend√≠ los conceptos de Kubernetes: pods, services, deployments",
					"Desarroll√© pipelines de CI/CD automatizados y confiables",
					"Aprend√≠ sobre observability: metrics, logs, traces",
					"Gan√© experiencia con incident response y postmortems"
				]
			},
			images: {
				title: "Capturas del Proyecto",
				items: [
					{
						placeholder: "üê≥",
						caption: "Arquitectura de containers con Docker Compose"
					},
					{
						placeholder: "‚ò∏Ô∏è",
						caption: "Kubernetes cluster con m√∫ltiples pods"
					},
					{
						placeholder: "üìä",
						caption: "Grafana dashboard con m√©tricas en tiempo real"
					}
				]
			}
		}
	},
	en: {
		"web-server": {
			overview: {
				title: "Overview",
				content: "This project emerged from the curiosity to understand how a web server works at a fundamental level. Instead of using popular frameworks like Express or FastAPI, I decided to build an HTTP server from scratch using native TCP/IP sockets. The goal was to understand every aspect of the HTTP protocol, from TCP handshake to request parsing and response generation."
			},
			challenge: {
				title: "The Challenge",
				content: "The biggest challenge was implementing concurrent handling of multiple connections without using third-party libraries. I had to deal with blocking vs non-blocking sockets, buffer management, and the complete lifecycle of an HTTP connection. I also learned about operating system limits regarding file descriptors and how they affect scalability."
			},
			solution: {
				title: "The Solution",
				content: "I implemented a multi-threaded server that accepts connections on the main thread and delegates them to workers for processing. Each worker manually parses the HTTP request, identifies the method and path, and generates an appropriate response. The server supports GET and POST requests, serving static files, and sending different status codes as needed."
			},
			technical: {
				title: "Technical Details",
				items: [
					"TCP/IP socket implementation using standard library",
					"Manual parsing of HTTP/1.1 protocol (request line, headers, body)",
					"Thread pool for concurrent connection handling",
					"Buffer management for data reading and writing",
					"Content-Type detection implementation for static files",
					"Keep-alive and connection close handling",
					"Testing with tools like curl and Apache Bench"
				]
			},
			learnings: {
				title: "Key Learnings",
				items: [
					"Gained deep understanding of client-server model and HTTP protocol",
					"Learned about systems programming and low-level I/O operations",
					"Understood advantages and limitations of different concurrency models",
					"Gained appreciation for complexity abstracted by modern frameworks",
					"Developed operating system-level debugging skills",
					"Learned about performance tuning and profiling network applications"
				]
			},
			images: {
				title: "Project Screenshots",
				items: [
					{
						placeholder: "üñ•Ô∏è",
						caption: "Terminal showing server accepting connections"
					},
					{
						placeholder: "üìä",
						caption: "Performance benchmarks with Apache Bench"
					},
					{
						placeholder: "üîç",
						caption: "HTTP Request/Response captured with Wireshark"
					}
				]
			}
		},
		"authentication": {
			overview: {
				title: "Overview",
				content: "A complete authentication system implementing modern security best practices. The project includes user registration, login, JWT token management, refresh tokens, and all necessary features for a robust production authentication system."
			},
			challenge: {
				title: "The Challenge",
				content: "Authentication is a critical area where mistakes have serious consequences. The challenge was to implement a secure system that protects against common vulnerabilities like: insecure password storage, timing attacks, session hijacking, CSRF, and XSS. It was also necessary to balance security with user experience."
			},
			solution: {
				title: "The Solution",
				content: "I implemented a dual-token system: short-lived access tokens for authentication and long-lived refresh tokens stored securely. Passwords are hashed with bcrypt using random salt. JWT tokens include custom claims and are signed with secure algorithms. The system also implements rate limiting and protection against brute force attacks."
			},
			technical: {
				title: "Technical Details",
				items: [
					"Password hashing with bcrypt (cost factor: 12)",
					"JWT with RS256 algorithm for asymmetric signing",
					"Access tokens (15 min) and Refresh tokens (7 days)",
					"Refresh token rotation for enhanced security",
					"Token whitelist in Redis for instant revocation",
					"Rate limiting per IP and per user",
					"Protection against timing attacks in password validation",
					"HTTP-only cookies for secure token storage",
					"CORS properly configured to prevent CSRF"
				]
			},
			learnings: {
				title: "Key Learnings",
				items: [
					"Understood differences between authentication and authorization",
					"Learned about applied cryptography: hashing, salting, and signing algorithms",
					"Understood complete JWT token flow and its trade-offs",
					"Learned about common attack vectors and how to mitigate them",
					"Developed judgment for evaluating security trade-offs",
					"Gained experience with Redis for distributed session management"
				]
			},
			images: {
				title: "Project Screenshots",
				items: [
					{
						placeholder: "üîê",
						caption: "Authentication system flow diagram"
					},
					{
						placeholder: "üé´",
						caption: "JWT payload structure with custom claims"
					},
					{
						placeholder: "‚ö†Ô∏è",
						caption: "Rate limiting in action blocking brute force attacks"
					}
				]
			}
		},
		"database": {
			overview: {
				title: "Overview",
				content: "This project explores relational database design and optimization. It includes everything from modeling normalized schemas to advanced optimization techniques like indexing, query optimization, and caching strategies. The focus is on understanding how design decisions affect performance in real applications."
			},
			challenge: {
				title: "The Challenge",
				content: "Designing a schema that is easy to maintain but also scales with millions of records. The challenge was identifying performance bottlenecks, understanding query plans, and applying optimizations without premature over-optimization. I also learned that caching can be a double-edged sword if not implemented correctly."
			},
			solution: {
				title: "The Solution",
				content: "I implemented a normalized schema in 3NF with strategically placed indexes on columns frequently used in WHEREs and JOINs. For complex queries, I use EXPLAIN ANALYZE to understand the query plan and optimize accordingly. I implemented caching at multiple levels: query results in Redis and materialized views for heavy aggregations."
			},
			technical: {
				title: "Technical Details",
				items: [
					"Normalized schema design (1NF, 2NF, 3NF) with selective denormalization",
					"B-tree indexes on frequently searched columns",
					"Composite indexes for multi-column queries",
					"Partial indexes for specific data subsets",
					"Materialized views for complex aggregations",
					"Query optimization using EXPLAIN ANALYZE",
					"Redis as cache layer with invalidation strategy",
					"Connection pooling with optimized configuration",
					"Large table partitioning by date"
				]
			},
			learnings: {
				title: "Key Learnings",
				items: [
					"Understood normalization theory and when to break the rules",
					"Learned to read query plans and identify costly operations",
					"Understood different index types and when to use each",
					"Developed intuition for read vs write trade-offs",
					"Learned about cache invalidation strategies and their challenges",
					"Gained experience with monitoring and profiling queries in production"
				]
			},
			images: {
				title: "Project Screenshots",
				items: [
					{
						placeholder: "üìê",
						caption: "ER diagram of database schema"
					},
					{
						placeholder: "üìà",
						caption: "Query performance before and after indexing"
					},
					{
						placeholder: "üîÑ",
						caption: "Multi-level caching architecture"
					}
				]
			}
		},
		"message-queue": {
			overview: {
				title: "Overview",
				content: "Implementation of a message queue system to decouple components and process tasks asynchronously. The project demonstrates how to use message queues to improve application scalability and resilience, handling heavy tasks like image processing, email sending, and background report generation."
			},
			challenge: {
				title: "The Challenge",
				content: "The biggest challenge was guaranteeing message delivery without losses or duplicates. I had to deal with workers that fail mid-processing, messages that repeatedly cause errors (poison messages), and maintaining processing order when important. It was also necessary to implement monitoring for queue state visibility."
			},
			solution: {
				title: "The Solution",
				content: "I implemented the system using RabbitMQ with multiple workers consuming messages from different queues. Each worker processes messages idempotently to handle redeliveries. For messages that fail repeatedly, I implemented a retry strategy with exponential backoff and a dead letter queue for later analysis."
			},
			technical: {
				title: "Technical Details",
				items: [
					"RabbitMQ as message broker with durability enabled",
					"Multiple workers with concurrent processing",
					"Manual acknowledgment to guarantee processing",
					"Retry logic with exponential backoff (1s, 2s, 4s, 8s...)",
					"Dead Letter Queue for persistently failing messages",
					"Priority queues for urgent messages",
					"Message persistence to disk for durability",
					"Monitoring with Prometheus and Grafana",
					"Idempotent processing to handle duplicates"
				]
			},
			learnings: {
				title: "Key Learnings",
				items: [
					"Understood messaging patterns: pub/sub, work queues, routing",
					"Learned about delivery guarantees: at-most-once, at-least-once, exactly-once",
					"Understood importance of idempotence in distributed systems",
					"Developed strategies for handling failures and poison messages",
					"Learned to balance throughput vs latency in workers",
					"Gained experience with observability in asynchronous systems"
				]
			},
			images: {
				title: "Project Screenshots",
				items: [
					{
						placeholder: "üìÆ",
						caption: "Message queue system architecture"
					},
					{
						placeholder: "üìä",
						caption: "Monitoring dashboard with queue metrics"
					},
					{
						placeholder: "üîÅ",
						caption: "Retry flow with exponential backoff"
					}
				]
			}
		},
		"rest-api": {
			overview: {
				title: "Overview",
				content: "A complete RESTful API following API design best practices. The project includes versioning, rate limiting, authentication, interactive documentation with OpenAPI/Swagger, and a robust error handling system. The goal is to create an API that is easy to use, maintain, and scale."
			},
			challenge: {
				title: "The Challenge",
				content: "Designing an API that is intuitive but also extensible. The challenge was making decisions about endpoint structure, versioning, and data contracts that don't break compatibility with existing clients. It was also necessary to implement effective rate limiting without affecting legitimate users and create documentation that stays synchronized with code."
			},
			solution: {
				title: "The Solution",
				content: "I implemented an API following REST principles with clearly defined resources and semantically correct HTTP methods. Versioning is done via URL (/v1/, /v2/). Rate limiting is implemented with Redis using sliding window algorithm. Documentation is automatically generated from code using decorators/annotations, ensuring it's always up to date."
			},
			technical: {
				title: "Technical Details",
				items: [
					"RESTful endpoints with resources and nested sub-resources",
					"API versioning via URL path (/api/v1/, /api/v2/)",
					"Rate limiting with sliding window algorithm in Redis",
					"Authentication with Bearer tokens (JWT)",
					"Automatically generated OpenAPI 3.0 documentation",
					"Swagger UI for interactive testing",
					"Consistent error handling with Problem Details (RFC 7807)",
					"Pagination with cursor-based pagination",
					"HATEOAS for discoverability",
					"Content negotiation (JSON, XML)"
				]
			},
			learnings: {
				title: "Key Learnings",
				items: [
					"Understood REST design principles and their advantages",
					"Learned about API versioning strategies and their trade-offs",
					"Understood different rate limiting algorithms and their characteristics",
					"Developed judgment for designing stable API contracts",
					"Learned importance of good documentation and developer experience",
					"Gained experience with OpenAPI/Swagger for API specifications"
				]
			},
			images: {
				title: "Project Screenshots",
				items: [
					{
						placeholder: "üìù",
						caption: "Swagger UI with interactive documentation"
					},
					{
						placeholder: "üö¶",
						caption: "Rate limiting in action protecting the API"
					},
					{
						placeholder: "üìã",
						caption: "Example response with Problem Details format"
					}
				]
			}
		},
		"devops": {
			overview: {
				title: "Overview",
				content: "This project covers the complete deployment and DevOps cycle for modern applications. It includes containerization with Docker, orchestration with Kubernetes, automated CI/CD pipelines, and production monitoring. The goal is to understand how to take code from development to production in an automated, secure, and reliable way."
			},
			challenge: {
				title: "The Challenge",
				content: "The biggest challenge was setting up a complete pipeline that is robust but not overly complex. I had to learn about multiple tools (Docker, Kubernetes, CI/CD, monitoring) and how to integrate them coherently. It was also necessary to implement zero-downtime deployments and automatic rollback when something fails."
			},
			solution: {
				title: "The Solution",
				content: "I containerized the application with Docker using multi-stage builds for size optimization. I implemented CI/CD with GitHub Actions that runs tests, build, and automatic deployment. Deployment to Kubernetes uses rolling updates for zero downtime. For monitoring, I configured Prometheus for metrics and Grafana for visualization, with automatic alerts when something goes wrong."
			},
			technical: {
				title: "Technical Details",
				items: [
					"Dockerfiles with multi-stage builds for optimization",
					"Docker Compose for local development with multiple services",
					"Kubernetes deployment with rolling updates and automatic rollback",
					"Health checks (liveness, readiness probes) for K8s",
					"CI/CD pipeline with GitHub Actions (test ‚Üí build ‚Üí deploy)",
					"GitOps workflow with automated deployments",
					"Prometheus for application metrics collection",
					"Grafana dashboards for metrics visualization",
					"Alertmanager for incident notifications",
					"Horizontal Pod Autoscaling based on metrics"
				]
			},
			learnings: {
				title: "Key Learnings",
				items: [
					"Understood complete cycle from development to production",
					"Learned about containerization and its advantages for deployment",
					"Understood Kubernetes concepts: pods, services, deployments",
					"Developed automated and reliable CI/CD pipelines",
					"Learned about observability: metrics, logs, traces",
					"Gained experience with incident response and postmortems"
				]
			},
			images: {
				title: "Project Screenshots",
				items: [
					{
						placeholder: "üê≥",
						caption: "Container architecture with Docker Compose"
					},
					{
						placeholder: "‚ò∏Ô∏è",
						caption: "Kubernetes cluster with multiple pods"
					},
					{
						placeholder: "üìä",
						caption: "Grafana dashboard with real-time metrics"
					}
				]
			}
		}
	}
};
